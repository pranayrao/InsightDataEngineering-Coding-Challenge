This test case carries out the following checks:
 1. Tweet with only one hashtag should not generate any edges and no new nodes are added to the graph.

 The third tweet has the value ['Apache'] in the hashtags field. This doesn't affect the graph in anyway and the average degree remains unchanged at 2.00
 
 2. The 60 second window is maintained.

 The sixth tweet has a created_at value of "Thu Nov 05 05:06:12 +0000 2015" and when it is processed the first tweet falls more than 60 seconds behind. Therefore the edges formed by the first tweet should be evicted.

 Prior to this tweet the edges were
 Spark <--> Apache
 Apache <--> Hadoop
 Hadoop <--> Storm
 Storm <--> Apache
 Flink <--> Spark
 HBase <--> Spark
 and the average degree was 2

 After the sixth tweet is processed the graph contains the following edges
 Apache <--> Hadoop
 Hadoop <--> Storm
 Storm <--> Apache
 Flink <--> Spark
 HBase <--> Spark
 and the average degree should be 1.66

 3. The code handles tweets that arrive out of order 
 The seventh tweet in the input file is out of order in time with a created_at value of "Thu Nov 05  05:06:10 +0000 2015" but falls within the 60 second time window of the maximum timestamp processed i.e., "Thu Nov 05  05:06:12 +0000 2015" and thus should be processed by the code adding the edges formed by it to the graph leading to the following edge list
 Apache <--> Hadoop
 Hadoop <--> Storm
 Storm <--> Apache
 Flink <--> Spark
 HBase <--> Spark
 HBase <--> Flink
 
 The resulting average degree will be 2.00
 
 4. The code ignores tweets that arrive out of order and are outside the 60 second window of the maximum created_at value.
 The eight tweet in the input file has a created at value of "Thu Nov 05  05:05:05 +0000 2015" which is more than 60 second behind the maximum processed created_at of "Thu Nov 05  05:06:12 +0000 2015"
 Thus no new nodes are added to the graph but the unchanged average degree is still written to the output file.